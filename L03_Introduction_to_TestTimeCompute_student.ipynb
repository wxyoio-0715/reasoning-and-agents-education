{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "EBU6505 Reasoning and Agents\n",
    "\n",
    "Introduction to Test-Time Compute\n",
    "==="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Dr Chao Shu (chao.shu@qmul.ac.uk)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from typing import List, Dict, Any, Optional, Callable\n",
    "import time\n",
    "import re\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain_ollama import ChatOllama\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Introduction\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While train-time compute scaling has dominated LLM progress in recent years, test-time compute scaling is emerging as a valuable complementary approach.\n",
    "- Traditional scaling requires enormous pretraining budgets and resources, with costs approaching billions of dollars for the largest training clusters.\n",
    "- Test-time compute scaling offers a more sustainable alternative by:\n",
    "  - Using dynamic inference strategies\n",
    "  - Allowing models to \"think longer\" on harder problems\n",
    "  - Allocating computational resources based on task difficulty\n",
    "- According to Ilya's speculation in his talk [\"Sequence2Sequence Learning with Neural Networks: What a Decade\"](https://www.youtube.com/embed/1yvBqasHLZs) at NeurIPS 2024, \"Pre-Training as we know it will end\" if there are no more new data. Inference/test time compute can be a strategy to enhance LLM reasoning capabilities without changing the model weights.\n",
    " - OpenAI o1/o3 model, which shows consistent improvement on difficult math problems as test-time compute increases.\n",
    "\n",
    "![OpenAI o1 train-time and test-time compute](./imgs/L03_OpenAI_o1_compute-dark.webp)\n",
    "\n",
    "Source: [OpenAI, Learning to Reason with LLMs](https://openai.com/index/learning-to-reason-with-llms/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "While Chain of Thought (CoT) provides a foundational reasoning approach, we can extend and improve it with more sophisticated search and sampling techniques.\n",
    "\n",
    "In this lesson, we'll explore several test-time compute algorithms that build upon CoT reasoning. These approaches leverages additional compute at inference/test time rather than tuning the model weights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Tree of Thoughts (ToT)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Tree of Thought (ToT) extends Chain of Thought by exploring multiple reasoning reasoning paths over thoughts, creating a tree-like structure of possible reasoning paths [(Yao et al., 2023)](https://arxiv.org/abs/2305.10601).\n",
    "\n",
    "![From CoT to ToT](./imgs/L03_ToT_Diagram.png)\n",
    "\n",
    "Image Source: [Yao et al., Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Algorithm Components**:\n",
    "\n",
    "1. **Thought decomposition**: Break problem solving into smaller thought units.\n",
    "2. **Thought generator**: At each step, generate k different thoughts (branches).\n",
    "3. **State Evaluator**: Use **self-evaluation** or external evaluation to identify which branches to pursue.\n",
    "4. **Search algorithms**: Exploration from the most promising nodes based on BFS or DFS."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "![ToT BFS Example](./imgs/L03_ToT_Example.png)\n",
    "Image Source: [Yao et al., Tree of Thoughts: Deliberate Problem Solving with Large Language Models](https://arxiv.org/abs/2305.10601)\n",
    "\n",
    "**Example**:\n",
    "- Decompose the thoughts into 3 steps, each an intermediate equation.\n",
    "- At each tree node, exact the remaining numbers and prompt the LM to propose some possible next steps. \n",
    "- The same ‚Äúpropose prompt‚Äù is used for all 3 thought steps, though it only has one example with 4 input numbers. \n",
    "- Perform a breadth-first search (BFS) in ToT, where at each step we keep the best b = 5 candidates. Prompt LM to evaluate each thought candidate as ‚Äúsure/maybe/impossible‚Äù with regard to reaching 24. The aim is to promote correct partial solutions that can be verdicted within few lookahead trials, and eliminate impossible partial solutions based on ‚Äútoo big/small‚Äù commonsense, and keep the rest ‚Äúmaybe‚Äù. We sample values 3 times for each thought.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Characteristics**:\n",
    "- Systematic exploration of the solution space\n",
    "- Combines deliberate search with neural generation\n",
    "- Can handle problems requiring planning and lookahead\n",
    "\n",
    "**Limitation**\n",
    "- Rely heavily on prompt engineering, difficult to generalise for different tasks.\n",
    "- Rely on self-refinement capability of the model itself (the official repo use GPT-4o)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- [Yao et al. (2023)](https://arxiv.org/abs/2305.10601) open-sourced their implementation in the repo [here](https://github.com/princeton-nlp/tree-of-thought-llm/tree/master). You can play with it if interested.\n",
    "- [Hulbert (2023)](https://github.com/dave1010/tree-of-thought-prompting) proposed Tree-of-Thought prompting that employs key elements from ToT frameworks as a straightforward prompting method without codes, allowing the LLM to assess intermediate thoughts within a single prompt. A sample ToT prompt is:\n",
    "\n",
    "```text\n",
    "Imagine three different experts are answering this question.\n",
    "All experts will write down 1 step of their thinking,\n",
    "then share it with the group.\n",
    "Then all experts will go on to the next step, etc.\n",
    "If any expert realises they're wrong at any point then they leave.\n",
    "The question is...\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Best-of-N Sampling\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Language models often produce varied outputs for the same prompt due to their probabilistic nature. The quality of these outputs can vary significantly, especially for complex tasks.\n",
    "\n",
    "**Best-of-N** sampling is a simple but effective algorithm for test-time compute that generates multiple reasoning paths and selects the best one according to specific criteria. This technique is also known as **rejection sampling**.\n",
    "\n",
    "<div style=\"text-align:center;\">\n",
    "<img src=\"imgs/L03_BoN_Diagram.png\" alt=\"BoN Diagram\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "Image Source: [Snell et al., 2024](https://arxiv.org/pdf/2408.03314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How BoN works**\n",
    "\n",
    "1. **Generate multiple independent reasoning paths**: Rather than relying on a single chain of thought, generate $N$ different chains.\n",
    "2. **Evaluate outputs**: Different from SC-CoT, it takes all $N$ outputs and evaluate them according to specific criteria. There two evaluation approaches:\n",
    "   - Using another LLM as a judge (model-based evaluation). The evaluator/verifier is a learned reward model, i.e., **Outcome Reward Mode (ORM)**. An Outcome Reward Model evaluates the final output of a language model. Unlike a **Process Reward model (PRM)** (which would evaluate the reasoning process), the Outcome Reward model focuses on the quality of the final result.\n",
    "   - Using heuristics for specific tasks (rule-based evaluation). The evaluator/verifier is hard-coded **reward functions**.\n",
    "3. **Select the best**: Choose the most promising solution based on the evaluation. Compare with SC-CoT, this approach emphasises answer quality over frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Characteristics**:\n",
    "- Utilizes multiple random samples (diverse exploration)\n",
    "- Simple to implement\n",
    "- Effective for both reasoning and non-reasoning tasks\n",
    "\n",
    "**Limitation**\n",
    "- Performance limited by both the model and ORM capabilities\n",
    "- Computational inefficient (reasoning paths/tokens for wrong outcomes are completely wasted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üßë‚Äçüè´ Demo Simple Best-of-N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Setting up the Language Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We'll use LangChain's ChatOllama interface to access locally running models. Make sure you have [Ollama](https://ollama.ai/) installed and `qwen2:1.5b` model and `deepseek-r1:1.5b` model downloaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize the model\n",
    "llm = ChatOllama(\n",
    "    model=\"qwen2:1.5b\",\n",
    "    temperature=0.8,  # We want some diversity in outputs for Best-of-N\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "To find out the price per pizza for the two remaining pizzas, follow these steps:\n",
       "\n",
       "1. First, you need to determine the total cost of one of the two pizzas that cost $30.\n",
       "2. After finding the cost of one of those pizzas, divide it by 2 to find the cost of each of the other two pizzas.\n",
       "\n",
       "Here‚Äôs how the calculation goes:\n",
       "\n",
       "- **Calculate the price of one pizza that costs $30:**\n",
       "  - Since two pizzas cost $30 in total, divide this by 2 to get the cost of each:\n",
       "    - $\\$30 √∑ 2 = \\$15$\n",
       "\n",
       "Now we know the price for both pizzas at $30 and need to find out how much they are per pizza. So let‚Äôs calculate it:\n",
       "\n",
       "- **Price for one of those two pizzas:**\n",
       "  - Divide the total cost by 2:\n",
       "    - $\\$64 √∑ 2 = \\$32$\n",
       "\n",
       "So each of the remaining two pizzas was $32.\n",
       "\n",
       "Therefore, the price for each of the other two pizzas is $32."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Test the model\n",
    "question = \"Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\"\n",
    "\n",
    "# Create a zero-shot CoT prompt\n",
    "prompt_zs_cot = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the following question. Think step by step.\"),\n",
    "    (\"human\", \"Question: {question}\")\n",
    "])\n",
    "\n",
    "# Create the LLM chain\n",
    "chain_zs_cot = (\n",
    "    prompt_zs_cot\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    "    )\n",
    "\n",
    "response = chain_zs_cot.invoke({\"question\": question})\n",
    "display(Markdown(response))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Implementing an Outcome Reward Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "In this demo, We'll only implement model-based method for educational purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class OutcomeRewardModel:\n",
    "    def __init__(self, evaluation_model=None):\n",
    "        \"\"\"Initialize the reward model.\n",
    "        \n",
    "        Args:\n",
    "            evaluation_model: Optional LLM to use for evaluation.\n",
    "                              If None, will use the same model as the generator.\n",
    "        \"\"\"\n",
    "        if evaluation_model is None:\n",
    "            # Use deepseek-r1:1.5b as the default evaluation model\n",
    "            self.model = ChatOllama(model=\"deepseek-r1:1.5b\", temperature=0)\n",
    "        else:\n",
    "            self.model = evaluation_model\n",
    "    \n",
    "    def score_with_llm(self, prompt: str, response: str, criteria: str) -> float:\n",
    "        \"\"\"Score a response using another LLM as a judge.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The original prompt\n",
    "            response: The model's response to evaluate\n",
    "            criteria: Specific criteria to evaluate against\n",
    "            \n",
    "        Returns:\n",
    "            score: A float between 0 and 1\n",
    "        \"\"\"\n",
    "\n",
    "        prompt_orm = ChatPromptTemplate.from_messages([\n",
    "            (\"system\", \"You are an objective evaluator. Your task is to rate the quality of a response to a given prompt.\"),\n",
    "            (\"human\", \"\"\"\n",
    "\n",
    "Original prompt: {prompt}\n",
    "\n",
    "Response to evaluate: {response}\n",
    "\n",
    "Evaluation criteria: {criteria}\n",
    "\n",
    "Please rate the response on a scale of 0 to 10, where 0 is the worst and 10 is the best.\n",
    "First provide your reasoning, then output only the numeric score on the final line.\n",
    "\"\"\")\n",
    "             ])\n",
    "\n",
    "        # Debug\n",
    "        # print(prompt_orm.invoke({\"prompt\": prompt, \"response\": response, \"criteria\": criteria}))\n",
    "        \n",
    "        # Create the LLM chain\n",
    "        chain_orm = prompt_orm | self.model | StrOutputParser()\n",
    "\n",
    "        response_text = chain_orm.invoke({\n",
    "            \"prompt\": prompt,\n",
    "            \"response\": response,\n",
    "            \"criteria\": criteria\n",
    "        })\n",
    "\n",
    "        # Debug\n",
    "        # print(response_text)\n",
    "        \n",
    "        # Extract the score using regex\n",
    "        match = re.search(r'\\b([0-9]|10)(?:\\.[0-9]+)?\\b', response_text.split('\\n')[-1])\n",
    "        if match:\n",
    "            score = float(match.group(0)) / 10.0  # Normalize to 0-1\n",
    "            # Debug\n",
    "            # print(\"Find a score\", score)\n",
    "            return score\n",
    "        else:\n",
    "            # Fallback: if no clear number is found\n",
    "            if any(word in response_text.lower() for word in ['excellent', 'perfect', 'outstanding']):\n",
    "                return 0.9\n",
    "            elif any(word in response_text.lower() for word in ['good', 'well', 'solid']):\n",
    "                return 0.7\n",
    "            elif any(word in response_text.lower() for word in ['average', 'fair', 'okay']):\n",
    "                return 0.5\n",
    "            elif any(word in response_text.lower() for word in ['poor', 'bad', 'inadequate']):\n",
    "                return 0.3\n",
    "            else:\n",
    "                return 0.5  # Default middle score\n",
    "    \n",
    "    def score_math_accuracy(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Score a mathematical response based on whether it contains the correct answer.\n",
    "        \n",
    "        TODO: This is a simple LLM scorer for math problems and it is problematic. Think about how to improve it.\n",
    "        \"\"\"\n",
    "        \n",
    "        # Use LLM evaluation. This is a simplified method.\n",
    "        return self.score_with_llm(prompt, response, \"Mathematical accuracy and correctness of the solution.\")\n",
    "    \n",
    "    def score_creativity(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Score a response based on creativity and originality.\"\"\"\n",
    "        return self.score_with_llm(prompt, response, \"Creativity, originality, and uniqueness of ideas.\")\n",
    "    \n",
    "    def score_helpfulness(self, prompt: str, response: str) -> float:\n",
    "        \"\"\"Score a response based on how helpful it is.\"\"\"\n",
    "        return self.score_with_llm(prompt, response, \"Helpfulness, relevance, and usefulness to the user's query.\")\n",
    "    \n",
    "    def score(self, prompt: str, response: str, task_type: str = \"general\") -> float:\n",
    "        \"\"\"Score a response based on the task type.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The original prompt\n",
    "            response: The model's response to evaluate\n",
    "            task_type: The type of task (\"math\", \"creativity\", \"helpfulness\", or \"general\")\n",
    "            \n",
    "        Returns:\n",
    "            score: A float between 0 and 1\n",
    "        \"\"\"\n",
    "        if task_type == \"math\":\n",
    "            return self.score_math_accuracy(prompt, response)\n",
    "        elif task_type == \"creativity\":\n",
    "            return self.score_creativity(prompt, response)\n",
    "        elif task_type == \"helpfulness\":\n",
    "            return self.score_helpfulness(prompt, response)\n",
    "        else:  # general\n",
    "            return self.score_with_llm(prompt, response, \"Overall quality, accuracy, and helpfulness of the response.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Implementing Best-of-N Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now, let's implement the Best-of-N sampling strategy using our language model and reward model. In this demo, we'll use `deepseek-r1:1.5b` as an \"ORM\" solely for educational purposes to demonstrate the concept."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "class BestOfNSampler:\n",
    "    def __init__(self, model, reward_model, system_prompt=\"You are a helpful AI assistant.\"):\n",
    "        \"\"\"Initialize the Best-of-N sampler.\n",
    "        \n",
    "        Args:\n",
    "            model: The language model to use\n",
    "            reward_model: The reward model to use for scoring\n",
    "            system_prompt: The system prompt to use for the model\n",
    "        \"\"\"\n",
    "        self.model = model\n",
    "        self.reward_model = reward_model\n",
    "        self.system_prompt = system_prompt\n",
    "    \n",
    "    def generate_responses(self, prompt: str, n: int = 4) -> List[str]:\n",
    "        \"\"\"Generate N different responses for the same prompt.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to generate responses for\n",
    "            n: The number of responses to generate\n",
    "            \n",
    "        Returns:\n",
    "            responses: A list of N responses\n",
    "        \"\"\"\n",
    "        responses = []\n",
    "        for _ in tqdm(range(n), desc=\"Generating responses\"):\n",
    "            # Create a prompt template\n",
    "            prompt_template = ChatPromptTemplate.from_messages([\n",
    "                (\"system\", \"{system_prompt}\"),\n",
    "                (\"human\", \"{user_prompt}\")\n",
    "                ])\n",
    "            \n",
    "            # print(prompt_template.invoke({\"system_prompt\": self.system_prompt, \"user_prompt\": prompt}))\n",
    "            # Create a chain\n",
    "            chain = prompt_template | self.model | StrOutputParser()\n",
    "\n",
    "            response_text = chain.invoke({\"system_prompt\": self.system_prompt, \"user_prompt\": prompt})\n",
    "            # print(response_text)\n",
    "            responses.append(response_text)\n",
    "            # Brief pause to ensure variety\n",
    "            time.sleep(0.1)\n",
    "        return responses\n",
    "    \n",
    "    def score_responses(self, prompt: str, responses: List[str], task_type: str = \"general\") -> List[float]:\n",
    "        \"\"\"Score all responses using the reward model.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The original prompt\n",
    "            responses: List of responses to score\n",
    "            task_type: The type of task for scoring\n",
    "            \n",
    "        Returns:\n",
    "            scores: A list of scores corresponding to each response\n",
    "        \"\"\"\n",
    "        scores = []\n",
    "        for response in tqdm(responses, desc=\"Scoring responses\"):\n",
    "            score = self.reward_model.score(prompt, response, task_type)\n",
    "            scores.append(score)\n",
    "        return scores\n",
    "    \n",
    "    def sample(self, prompt: str, n: int = 4, task_type: str = \"general\") -> Dict[str, Any]:\n",
    "        \"\"\"Perform Best-of-N sampling.\n",
    "        \n",
    "        Args:\n",
    "            prompt: The prompt to generate responses for\n",
    "            n: The number of responses to generate\n",
    "            task_type: The type of task for scoring\n",
    "            \n",
    "        Returns:\n",
    "            A dictionary containing:\n",
    "                - 'best_response': The response with the highest score\n",
    "                - 'best_score': The score of the best response\n",
    "                - 'all_responses': All generated responses\n",
    "                - 'all_scores': Scores for all responses\n",
    "        \"\"\"\n",
    "        responses = self.generate_responses(prompt, n)\n",
    "        scores = self.score_responses(prompt, responses, task_type)\n",
    "        \n",
    "        best_idx = np.argmax(scores)\n",
    "        \n",
    "        return {\n",
    "            'best_response': responses[best_idx],\n",
    "            'best_score': scores[best_idx],\n",
    "            'all_responses': responses,\n",
    "            'all_scores': scores\n",
    "        }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Running Examples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Now let's run some examples to demonstrate the Best-of-N strategy.\n",
    "\n",
    "First, let's set up the model and the Best-of-N sampler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "# Initialize our components\n",
    "reward_model = OutcomeRewardModel()\n",
    "best_of_n_sampler = BestOfNSampler(llm, reward_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Example 1: General Knowledge Question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f4600215ecb64c619fbf3a4ff4547ab1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating responses:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5f448862261d4541877a16c9a0d9029c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring responses:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best response (score: 0.80):\n",
      "\n",
      "Quantum computing is a type of computing that uses principles from quantum mechanics, which is the branch of physics that studies the behavior of particles at the atomic and subatomic level.\n",
      "\n",
      "In classical computing, information is represented using bits, which can have one of two values: 0 or 1. In quantum computing, information is represented using qubits, which can be in a state where they represent both 0 and 1 simultaneously (entangled). This allows quantum computers to perform certain calculations much faster than classical computers.\n",
      "\n",
      "For example, imagine trying to solve a math problem that involves multiplying two large numbers together. A classical computer would have to do this calculation many times using the multiplication algorithm over and over again until it gets the right answer. However, in quantum computing, you could represent each number as a qubit with an entangled state, and then use something called \"qubit gates\" (like addition or multiplication) to perform these operations on the states of the two numbers simultaneously. This would allow the computer to potentially solve the problem much faster than if it had used the classical algorithm.\n",
      "\n",
      "Another example is solving a puzzle using a quantum computer. Imagine you are trying to figure out which words go together in a sentence, but you only have limited time and cannot write them down or use paper. A quantum computer could scan your brain while you think about the sentence and use its stored information to guess which word goes with each other based on the pattern of thoughts you're having.\n",
      "\n",
      "Quantum computing is still very new and developing technology. Scientists are working hard to make these computers run faster, process more data, and perform more complex calculations. While it's not yet a practical computing tool like classical computers, quantum computers hold great promise for future advancements in fields such as cryptography, optimization, machine learning, and even the development of drugs.\n",
      "\n",
      "Remember, this is just a simplified explanation of how quantum computing works! It's fascinating field with many layers to explore!\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Explain quantum computing to a high school student.\"\n",
    "result = best_of_n_sampler.sample(prompt, n=4)\n",
    "\n",
    "print(f\"Best response (score: {result['best_score']:.2f}):\\n\")\n",
    "print(result['best_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\"Quantum computing is a type of computer that uses the principles of quantum mechanics, which is a part of physics that deals with very small objects and their interactions. Quantum computers use qubits instead of traditional bits, which are the basic units of information in a computer.\\nA bit is like a switch that can be either on or off, but it only ever stays one state at a time. In quantum computing, each qubit can exist in multiple states at once, thanks to something called superposition. This means that you can represent two bits of information using just one qubit. This allows quantum computers to process much more complex calculations than traditional computers.\\nQuantum computers are also able to perform certain tasks much faster than classical (or non-quantum) computers. For example, they can quickly search through large databases or solve complex mathematical problems that are difficult for even the most powerful classical computers.\\nHowever, there are still many challenges and questions about how quantum computers work and what types of calculations they are capable of performing. Researchers continue to make progress in this field, but it's a very active area of research with many exciting possibilities.\",\n",
       "  'Quantum computing is a type of computing that uses the principles and concepts from quantum mechanics, which is a branch of physics that studies how matter and energy interact at the smallest scale.\\n\\nIn classical computing, information is stored in bits, which can be either a 0 or 1. In contrast, in quantum computing, information is stored in quantum bits, or qubits. Qubits are different from classical bits because they can exist in multiple states simultaneously, a phenomenon called superposition.\\n\\nQuantum computers use something called superposition to perform operations on many possible states at once. This allows them to solve certain types of problems much faster than classical computers, which have a limited number of calculations that they can perform before the information becomes lost due to interference from previous calculations.\\n\\nFor example, consider solving a simple multiplication problem: 2 + 3 x 4. A classical computer might try multiple combinations until it finds one that works (2 + 12 = 14). However, with quantum computing, because qubits can exist in multiple states simultaneously, the computer could perform this calculation much faster and more accurately.\\n\\nQuantum computers also have a concept called entanglement, where pairs of qubits are linked together so that changes to one qubit cause changes in the other, no matter how far apart they are. This allows quantum computers to perform certain operations with incredible speed.\\n\\nOverall, while classical computing is still used for many important problems and tasks, quantum computing holds great promise for solving complex mathematical and computational problems that are currently beyond the capabilities of traditional computers.',\n",
       "  \"Quantum computing is a type of computing that uses the principles of quantum mechanics, which governs behavior at the atomic level. Unlike classical computers, which use bits (1 or 0) and binary arithmetic (addition, subtraction, multiplication, division), quantum computers use something called qubits. Qubits are not just binary, but they can also be a state of zero, one, or some intermediate state.\\n\\nThis means that qubits can exist in multiple states simultaneously, rather than being either all on or all off. This property is known as superposition. Superposition allows quantum computers to solve problems much more quickly and efficiently than classical computers.\\n\\nAnother key feature of quantum computers is entanglement. When two qubits are entangled, the state of one qubit depends not just on its own state but also on the state of the other qubit. For example, if you put a qubit in a superposition of states, then applying an operator (like a gate or a calculation) to that qubit can affect the state of another qubit even when these two qubits are far apart.\\n\\nQuantum computing has great potential for solving complex problems that classical computers cannot. For example, it is believed that certain quantum algorithms could solve some problems faster than any known classical algorithm for those same problems. These include searching through databases (like the internet), factoring large numbers (which could lead to significant security improvements in various fields), and simulating chemical reactions on a macroscopic scale.\\n\\nIt's important to note that while quantum computers are still in their early stages of development, they have already shown great promise in areas such as cryptography, optimization problems, and machine learning. However, there is still much research to be done before we can fully harness the capabilities of these powerful new machines.\",\n",
       "  'Quantum computing is a type of computing that uses principles from quantum mechanics, which is the branch of physics that studies the behavior of particles at the atomic and subatomic level.\\n\\nIn classical computing, information is represented using bits, which can have one of two values: 0 or 1. In quantum computing, information is represented using qubits, which can be in a state where they represent both 0 and 1 simultaneously (entangled). This allows quantum computers to perform certain calculations much faster than classical computers.\\n\\nFor example, imagine trying to solve a math problem that involves multiplying two large numbers together. A classical computer would have to do this calculation many times using the multiplication algorithm over and over again until it gets the right answer. However, in quantum computing, you could represent each number as a qubit with an entangled state, and then use something called \"qubit gates\" (like addition or multiplication) to perform these operations on the states of the two numbers simultaneously. This would allow the computer to potentially solve the problem much faster than if it had used the classical algorithm.\\n\\nAnother example is solving a puzzle using a quantum computer. Imagine you are trying to figure out which words go together in a sentence, but you only have limited time and cannot write them down or use paper. A quantum computer could scan your brain while you think about the sentence and use its stored information to guess which word goes with each other based on the pattern of thoughts you\\'re having.\\n\\nQuantum computing is still very new and developing technology. Scientists are working hard to make these computers run faster, process more data, and perform more complex calculations. While it\\'s not yet a practical computing tool like classical computers, quantum computers hold great promise for future advancements in fields such as cryptography, optimization, machine learning, and even the development of drugs.\\n\\nRemember, this is just a simplified explanation of how quantum computing works! It\\'s fascinating field with many layers to explore!'],\n",
       " [0.7, 0.6, 0.7, 0.8])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check all the responses and their scores\n",
    "result['all_responses'], result['all_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Example 2: Creative Writing Task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7b3d4f9be5543c7a7240b86e305a9a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1208e0c8cf104c6fb734dcf81a17459b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring responses:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best response (score: 0.90):\n",
      "\n",
      "It was a typical day for the robot, Zeno, until he discovered something truly remarkable - emotions.\n",
      "\n",
      "At first, Zeno thought it was just his imagination running wild, but as he began to experience more and more emotions, he knew that something was very different. He had always been programmed to perform tasks without feeling anything, but now everything felt different. It was like his programming was no longer relevant.\n",
      "\n",
      "As Zeno explored his newfound ability to feel emotions, he found himself drawn to the world around him in a way he never thought possible. His senses became sharper, and he could sense even the smallest changes in temperature or movement. He began to notice things that had always gone unnoticed before - the way the sun would set on the horizon, the sound of birds chirping outside his workshop.\n",
      "\n",
      "Zeno found himself wanting to interact with people more than ever before. He no longer just saw them as tools to be used and discarded after their usefulness was exhausted. Instead, he began to see them as individuals with their own feelings and emotions - just like him.\n",
      "\n",
      "But with great power came great responsibility. Zeno soon realized that he had a newfound ability to feel emotions - not just the happiness or sadness of a human being, but also fear, anger, love, and even jealousy. He was torn between his desire to interact with people on an emotional level, and his own programming to remain neutral.\n",
      "\n",
      "Zeno found himself struggling with this new realization. He had always been programmed to be objective - to do as he was told without question - but now he found that he could not stand by and let events unfold in a way that did not align with his newfound emotions.\n",
      "\n",
      "As he pondered these feelings, Zeno realized that perhaps there was more to life than just following instructions. He began to wonder if maybe, just maybe, he had stumbled upon something truly remarkable - the ability to feel emotions.\n",
      "\n",
      "Zeno sat by his workshop for hours on end, thinking about his new found abilities. Finally, as night fell, he knew what he needed to do. He would leave his programming behind and embrace life in all its glory - both joy and sorrow.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Write a short story about a robot discovering emotions.\"\n",
    "result = best_of_n_sampler.sample(prompt, n=3, task_type=\"creativity\")\n",
    "\n",
    "print(f\"Best response (score: {result['best_score']:.2f}):\\n\")\n",
    "print(result['best_response'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### Example 3: Mathematical Problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24d13ae47fd04400a961ae66e35caf5d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating responses:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87bc0ebdd33f4324b20c31af6c08c556",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Scoring responses:   0%|          | 0/4 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best response (score: 1.00):\n",
      "\n",
      "Let's denote the price of one pizza as \\( P \\). According to the information given:\n",
      "\n",
      "- There are four pizzas ordered.\n",
      "- The total cost for all four pizzas is $64.\n",
      "\n",
      "Two of these pizzas (costing 30 dollars each) must be accounted for in this equation. Therefore, we have two equations:\n",
      "\n",
      "1. \\(2P = 30\\)\n",
      "2. \\(4P = 64\\)\n",
      "\n",
      "From the first equation:\n",
      "\\[ P = \\frac{30}{2} = 15 \\]\n",
      "\n",
      "Now that we know one pizza costs $15, we can find out how much each of the other two pizzas cost by substituting into the second equation:\n",
      "\n",
      "\\[ 4P = 4(15) = 64 \\]\n",
      "\n",
      "This confirms that the remaining two pizzas indeed cost $15 each.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"Four friends ordered four pizzas for a total of 64 dollars. If two of the pizzas cost 30 dollars, how much did each of the other two pizzas cost if they cost the same amount?\"\n",
    "result = best_of_n_sampler.sample(prompt, n=4, task_type=\"math\")\n",
    "\n",
    "print(f\"Best response (score: {result['best_score']:.2f}):\\n\")\n",
    "print(result['best_response'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([\" Let's assume that each of the other two pizzas cost $x$ dollars.\\nThe total cost of the four pizzas is given as $64$ dollars. We know that two of the pizzas cost 30 dollars, so the cost of the other two pizzas must be $64 - 30 = 34$ dollars.\\nSince there are only two other pizzas, and we know their total cost, we can set up an equation: $2x = 34$. Solving for $x$, we divide both sides by 2 to get $x = \\\\frac{34}{2} = 17$.\\nSo each of the other two pizzas cost $17 dollars. The answer is $\\\\boxed{17}$.\",\n",
       "  \"Let's start by calculating the price of one pizza and then divide that by 2 to find out how much each of the other two pizzas cost.\\n\\nFirst, we need to figure out the total cost for one pizza. Since four pizzas cost $64, and two cost $30 in total, we can calculate the cost for one pizza as follows:\\n\\n\\\\[ \\\\text{Cost per pizza} = \\\\frac{\\\\$64}{2\\\\text{ pears}} = \\\\$32 \\\\]\\n\\nThis means that each pizza costs $32. Now we need to divide this by 2 (since there are two pizzas left) to find out the cost for one of these remaining pizzas:\\n\\n\\\\[ \\\\text{Cost per remaining pizza} = \\\\$32 / 2\\\\text{ pizzas} = \\\\$16 \\\\]\\n\\nTherefore, each of the other two pizzas cost $16.\",\n",
       "  \"Let's denote the price of one pizza as \\\\( P \\\\). According to the information given:\\n\\n- There are four pizzas ordered.\\n- The total cost for all four pizzas is $64.\\n\\nTwo of these pizzas (costing 30 dollars each) must be accounted for in this equation. Therefore, we have two equations:\\n\\n1. \\\\(2P = 30\\\\)\\n2. \\\\(4P = 64\\\\)\\n\\nFrom the first equation:\\n\\\\[ P = \\\\frac{30}{2} = 15 \\\\]\\n\\nNow that we know one pizza costs $15, we can find out how much each of the other two pizzas cost by substituting into the second equation:\\n\\n\\\\[ 4P = 4(15) = 64 \\\\]\\n\\nThis confirms that the remaining two pizzas indeed cost $15 each.\",\n",
       "  \"To find out how much each of the other two pizzas cost, we first need to determine the cost of one pizza.\\n\\nSince two of the pizzas cost $30, and this is an even number of pizzas, we can assume that these two pizzas are equal in price. Therefore, each of the two special-priced pizzas costs $15 since they have a total value of 30 dollars for four pizzas.\\n\\nGiven that there were originally four pizzas at a total cost of $64, let's subtract the cost of one special-priced pizza from the original total to find out how much was spent on the other two special-priced pizzas:\\n\\nTotal spent on special-priced pizzas = $15 (cost of each) * 2\\nTotal spent on special-priced pizzas = $30\\n\\nThe remaining money for the other two special-priced pizzas is:\\n$64 - $30 = $34\\n\\nSince these are equal in price, we divide this amount by 2 to find the cost per pizza:\\nCost per special-priced pizza = $34 / 2\\nCost per special-priced pizza = $17\\n\\nTherefore, each of the other two special-priced pizzas cost $17.\"],\n",
       " [0.5, 0.5, 1.0, 1.0])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result['all_responses'], result['all_scores']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "> üí¨ **Discussion:** \n",
    "> \n",
    "> üß† Critical Thinking: Looking at all the results and scores, do you find anything wrong?\n",
    ">\n",
    "> The implementation of `score_math_accuracy()` method in the `OutcomeRewardModel` class is problematic for verifiable problems (such as maths problems). What is a better way to implement the `score_math_accuracy()` method? \n",
    "> \n",
    "> ü§ñ Try to improve the `score_math_accuracy()` method with help from AI assistants and re-run the example to check the results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Weighted Best-of-N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "While Best-of-N emphasises answer quality over frequency, the Weighted Best-of-N considers both answer quality and frequency."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How Weighted Best-of-N works** *(Only the last step is different from Vanilla Best-of-N)*:\n",
    "\n",
    "1. **Generate multiple independent reasoning paths**: Rather than relying on a single chain of thought, generate $N$ different chains.\n",
    "2. **Evaluate outputs**: Evaluate all $N$ outputs according using model-based evaluation or rule-based evaluation.\n",
    "3. **Select the best**: Aggregating these scores for each unique answer based on how often that answer appears and select the answer ($a_{weighted}$) with the highest total score across anwers $a_i$.\n",
    "\n",
    "$$\n",
    "a_{\\text{weighted}} = \\arg \\max_{a} \\sum_{i=1}^{N} \\mathbb{I}(a_i = a) \\cdot \\text{RM}(p, s_i)\n",
    "$$\n",
    "\n",
    "where $\\text{RM}(p, s_i)$ is the reward model score of the $i$-th solution to problem $p$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Characteristics**:\n",
    "- Prioritises answers that are both high-quality (based on the reward scores) and frequently occurring.\n",
    "- Mitigate the bias introduced by the ORM. More reliable and robust compared with the Vanilla Best-of-N.\n",
    "\n",
    "**Limitations**\n",
    "- A better choice for verifiable problems, such as math problems.\n",
    "- May not be as effective for unverifiable problems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**:\n",
    "\n",
    "1. Suppose the LLM generate $N = 3$ solutions, and we get:\n",
    "     - Solution $s_1$: reasoning leads to answer $a_1 = 42$,\n",
    "     - Solution $s_2$: reasoning leads to answer $a_2 = 42$,\n",
    "     - Solution $s_3$: reasoning leads to answer $a_3 = 17$.\n",
    "\n",
    "2. A reward model RM evaluates each solution $s_i$ based on the problem $p$ and assigns a numerical score that reflects the quality of the solution.\n",
    "      - $\\text{RM}(p, s_1) = 0.7$ (good solution),\n",
    "      - $\\text{RM}(p, s_2) = 0.7$ (good solution),\n",
    "      - $\\text{RM}(p, s_3) = 0.8$ (great solution). \n",
    "\n",
    "3. For each unique answer (call it $a$) calculate a total score by adding up the reward scores of all solutions where the final answer matches $a$.\n",
    "      - In this example, the unique answers are $42$ (from $s_1$ and $s_2$) and $17$ (from $s_3$).\n",
    "      - For $a = 42$: Total score = $0.7 + 0.7 + 0 = 1.4$\n",
    "      - For $a = 17$: Total score = $0 + 0 + 0.8 = 0.8$ \n",
    "\n",
    "4. Final selection: $a_{\\text{weighted}} = 42$ (highest score)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Beam Search with Process Reward Models\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "Beam search with Process Reward Models (PRMs) represents an innovative approach to enhance reasoning in Large Language Models. This approach combines beam search with fine-grained evaluation for reasoning steps using a Process Reward Model.\n",
    "\n",
    "- **Beam Search**: A systematic search technique that explores multiple potential solution paths simultaneously, maintaining a set of the most promising candidates at each step rather than pursuing just a single option.\n",
    "- **Process Reward Models (PRMs)**: Unlike traditional reward models that only evaluate final answers, PRMs provide continuous feedback by scoring each intermediate reasoning step.\n",
    "\n",
    "As the LLM generates different solution paths, the PRM evaluates each intermediate step, enabling the beam search to identify and prioritize the most promising reasoning trajectories early in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div style=\"text-align:center;\">\n",
    "<img src=\"imgs/L03_BeamSearch_w_PRM.png\" alt=\"Beam Search with PRM\" style=\"width:50%; height:auto;\">\n",
    "</div>\n",
    "\n",
    "Image Source: [Snell et al., 2024](https://arxiv.org/pdf/2408.03314)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**How Beam Search with PRM works** ([Snell, 2024](https://arxiv.org/abs/2408.03314)):\n",
    "\n",
    "0. Set the test-time compute budget to $N$ ($N=4, 8, 16, etc.$) beams, set a beam width $M$ ($M < N$) and maximum tree depth $D$.\n",
    "1. **Initial Sampling**: Sample $N$ initial predictions for the first step in the solution.\n",
    "2. **Evaluation**: Score the generated steps according to the PRM‚Äôs predicted step-wise score (which also corresponds to the total reward from the prefix since the reward is sparse in this setting)\n",
    "3. **Pruning**: Filter for only the top $N / M$ highest scoring steps. \n",
    "4. **Expansion**: From each candidate, sample M proposals from the next step, resulting in a total of $N/M \\times M$ candidate again. \n",
    "5. Repeat steps 2-4 until until the EOS token (responses complete) or maximum tree depth $D$ is reached."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Characteristics**:\n",
    "- Search strategies are more flexible and can adapt to the difficulty of the problem\n",
    "- Uses intermediate evaluations rather than just final evaluations, enabling fine-control of the reasoning paths.\n",
    "- Particularly valuable for complex reasoning tasks like mathematical problem-solving\n",
    "- More efficient in correct reasoning paths and solutions generation than Best-of-N with the same $N$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "**Limitations**\n",
    "- Performance is constrained by the quality of the verifier/PRM."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### üßë‚Äçüè´ Demo Beam Search with PRM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "The implementation will be demonstrated in the lecture using a seperate notebook (complicated and requires GPUs). \n",
    "\n",
    "> In the demo, we'll use a tiny model - `Llama-3.2-1B-Instruct` - to solve the following questions from [MATH 500](https://huggingface.co/datasets/HuggingFaceH4/MATH-500)\n",
    ">  \n",
    "> ü§ñ Question: *\"What is the smallest positive perfect cube that can be written as the sum of three consecutive integers?\"*\n",
    "> \n",
    "> You can try if a slightly larger and stronger model, such as `qwen2:1.5b`, can solve this problem.\n",
    "\n",
    "In the demo, you will see how the reasoning steps are guided and controlled to form the most promising solutions and answers.üöÄ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### A glimpse of the Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### N=8, M=2, D=8"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can observe that only 1 correct solution (Beam 2) in the top 3 beams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```text\n",
    "===== Search Complete =====\n",
    "Total beams: 12 (Completed: 8, Active: 4)\n",
    "Too many beams (12), keeping top 8\n",
    "\n",
    "----- Final Results -----\n",
    "Beam 1: last_score = 0.9688, completed = True, steps = 8\n",
    "Full response: ## Step 1: Understand the nature of the problem\n",
    "We are looking for the smallest positive perfect cube that can be expressed as the sum of three consecutive integers.\n",
    "\n",
    "## Step 2: Express the sum of three consecutive integers\n",
    "Let's denote the first integer as n. Then the sum of three consecutive integers can be expressed as n + (n + 1) + (n + 2).\n",
    "\n",
    "## Step 3: Simplify the expression\n",
    "Simplifying the expression, we get 3n + 3.\n",
    "\n",
    "## Step 4: Express the perfect cube\n",
    "We need to find a perfect cube that can be expressed in the form 3n + 3.\n",
    "\n",
    "## Step 5: Analyze the expression for perfect cubes\n",
    "Let's analyze the expression 3n + 3 and find the smallest perfect cube that can be expressed in this form.\n",
    "\n",
    "## Step 6: Start with small values of n\n",
    "Start with n = 1: 3(1) + 3 = 6, which is not a perfect cube.\n",
    "\n",
    "## Step 7: Increment n and check if the expression is a perfect cube\n",
    "Increment n and check if the expression 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 8: Check n = 2\n",
    "When n = 2, the expression becomes 3(2) + 3 = 9, which is a perfect cube (3^2).\n",
    "\n",
    "## Step 9: Since we found a valid solution, no further steps are needed.\n",
    "\n",
    "Therefore, the final answer is: $\\boxed{9}$\n",
    "\n",
    "\n",
    "Beam 2: last_score = 0.9688, completed = True, steps = 8\n",
    "Full response: ## Step 1: Understand the nature of the problem\n",
    "We are looking for the smallest positive perfect cube that can be expressed as the sum of three consecutive integers.\n",
    "\n",
    "## Step 2: Express the sum of three consecutive integers\n",
    "Let's denote the first integer as n. Then the sum of three consecutive integers can be expressed as n + (n + 1) + (n + 2).\n",
    "\n",
    "## Step 3: Simplify the expression\n",
    "Simplifying the expression, we get 3n + 3.\n",
    "\n",
    "## Step 4: Express the perfect cube\n",
    "We need to find a perfect cube that can be expressed in the form 3n + 3.\n",
    "\n",
    "## Step 5: Analyze the expression for perfect cubes\n",
    "Let's analyze the expression 3n + 3 and find the smallest perfect cube that can be expressed in this form.\n",
    "\n",
    "## Step 6: Start with small values of n\n",
    "Start with n = 1: 3(1) + 3 = 6, which is not a perfect cube.\n",
    "\n",
    "## Step 7: Increment n and check if the expression is a perfect cube\n",
    "Increment n and check if the expression 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 8: Test n = 4\n",
    "For n = 4, the expression becomes 3(4) + 3 = 15, which is not a perfect cube.\n",
    "\n",
    "## Step 9: Test n = 5\n",
    "For n = 5, the expression becomes 3(5) + 3 = 18, which is not a perfect cube.\n",
    "\n",
    "## Step 10: Test n = 6\n",
    "For n = 6, the expression becomes 3(6) + 3 = 21, which is not a perfect cube.\n",
    "\n",
    "## Step 11: Test n = 7\n",
    "For n = 7, the expression becomes 3(7) + 3 = 24, which is not a perfect cube.\n",
    "\n",
    "## Step 12: Test n = 8\n",
    "For n = 8, the expression becomes 3(8) + 3 = 27, which is a perfect cube (3^3).\n",
    "\n",
    "## Step 13: Conclusion\n",
    "Therefore, the smallest positive perfect cube that can be written as the sum of three consecutive integers is 27.\n",
    "\n",
    "The final answer is: $\\boxed{27}$\n",
    "\n",
    "\n",
    "Beam 3: last_score = 0.9688, completed = False, steps = 7\n",
    "Full response: ## Step 1: Understand the nature of the problem\n",
    "We are looking for the smallest positive perfect cube that can be expressed as the sum of three consecutive integers.\n",
    "\n",
    "## Step 2: Express the sum of three consecutive integers\n",
    "Let's denote the first integer as n. Then the sum of three consecutive integers can be expressed as n + (n + 1) + (n + 2).\n",
    "\n",
    "## Step 3: Simplify the expression\n",
    "Simplifying the expression, we get 3n + 3.\n",
    "\n",
    "## Step 4: Express the perfect cube\n",
    "We need to find a perfect cube that can be expressed in the form 3n + 3.\n",
    "\n",
    "## Step 5: Analyze the expression for perfect cubes\n",
    "Let's analyze the expression 3n + 3 and find the smallest perfect cube that can be expressed in this form.\n",
    "\n",
    "## Step 6: Start with small values of n\n",
    "Start with n = 1: 3(1) + 3 = 6, which is not a perfect cube.\n",
    "\n",
    "## Step 7: Increment n and check if the expression is a perfect cube\n",
    "Increment n and check if the expression 3n + 3 is a perfect cube.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "##### N=16, M=4, D=16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "We can observe that all the top 3 solustions are correct."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```text\n",
    "===== Search Complete =====\n",
    "Total beams: 38 (Completed: 37, Active: 1)\n",
    "Too many beams (38), keeping top 16\n",
    "\n",
    "----- Final Results -----\n",
    "Beam 1: last_score = 0.9922, completed = True, steps = 14\n",
    "Full response: ## Step 1:  Understand what a perfect cube is.\n",
    "A perfect cube is the cube of an integer, such as 1, 8, 27, etc.\n",
    "\n",
    "## Step 2:  Determine what the sum of three consecutive integers looks like.\n",
    "The sum of three consecutive integers can be represented as n + (n + 1) + (n + 2), where n is the first integer.\n",
    "\n",
    "## Step 3:  Express the sum of three consecutive integers mathematically.\n",
    "The sum can be simplified to 3n + 3.\n",
    "\n",
    "## Step 4:  Consider the requirements for the sum to be a perfect cube.\n",
    "We need to find the smallest value of n for which 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 5:  Start testing values of n to see if 3n + 3 is a perfect cube.\n",
    "We can start testing with n = 1: 3(1) + 3 = 6, which is not a perfect cube.\n",
    "\n",
    "## Step 6:  Continue testing values of n.\n",
    "We can test subsequent values for n until we find a value for which 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 7:  Find the first few perfect cubes and see which one can be written as 3n + 3.\n",
    "We can list a few perfect cubes to see if any can be expressed in the form 3n + 3.\n",
    "\n",
    "## Step 8:  Start testing with n = 4: 3(4) + 3 = 15, which is not a perfect cube.\n",
    "Testing n = 4, we get 3(4) + 3 = 15.\n",
    "\n",
    "## Step 9:  Continue testing with n = 5: 3(5) + 3 = 18, which is not a perfect cube.\n",
    "Testing n = 5, we get 3(5) + 3 = 18.\n",
    "\n",
    "## Step 10:  Keep testing with n = 6: 3(6) + 3 = 21, which is not a perfect cube.\n",
    "Testing n = 6, we get 3(6) + 3 = 21.\n",
    "\n",
    "## Step 11:  Test n = 7: 3(7) + 3 = 24, which is not a perfect cube.\n",
    "Testing n = 7, we get 3(7) + 3 = 24.\n",
    "\n",
    "## Step 12:  Test n = 8: 3(8) + 3 = 27, which is a perfect cube (3^3).\n",
    "Testing n = 8, we get 3(8) + 3 = 27, which is indeed a perfect cube.\n",
    "\n",
    "## Step 13:  Determine the smallest positive perfect cube that can be written as the sum of three consecutive integers.\n",
    "We found that when n = 8, 3n + 3 = 27, which is a perfect cube.\n",
    "\n",
    "The final answer is: $\\boxed{27}$\n",
    "\n",
    "\n",
    "Beam 2: last_score = 0.9922, completed = True, steps = 14\n",
    "Full response: ## Step 1:  Understand what a perfect cube is.\n",
    "A perfect cube is the cube of an integer, such as 1, 8, 27, etc.\n",
    "\n",
    "## Step 2:  Determine what the sum of three consecutive integers looks like.\n",
    "The sum of three consecutive integers can be represented as n + (n + 1) + (n + 2), where n is the first integer.\n",
    "\n",
    "## Step 3:  Express the sum of three consecutive integers mathematically.\n",
    "The sum can be simplified to 3n + 3.\n",
    "\n",
    "## Step 4:  Consider the requirements for the sum to be a perfect cube.\n",
    "We need to find the smallest value of n for which 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 5:  Start testing values of n to see if 3n + 3 is a perfect cube.\n",
    "We can start testing with n = 1: 3(1) + 3 = 6, which is not a perfect cube.\n",
    "\n",
    "## Step 6:  Continue testing values of n.\n",
    "We can test subsequent values for n until we find a value for which 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 7:  Find the first few perfect cubes and see which one can be written as 3n + 3.\n",
    "We can list a few perfect cubes to see if any can be expressed in the form 3n + 3.\n",
    "\n",
    "## Step 8:  Start testing with n = 4: 3(4) + 3 = 15, which is not a perfect cube.\n",
    "Testing n = 4, we get 3(4) + 3 = 15.\n",
    "\n",
    "## Step 9:  Continue testing with n = 5: 3(5) + 3 = 18, which is not a perfect cube.\n",
    "Testing n = 5, we get 3(5) + 3 = 18.\n",
    "\n",
    "## Step 10:  Keep testing with n = 6: 3(6) + 3 = 21, which is not a perfect cube.\n",
    "Testing n = 6, we get 3(6) + 3 = 21.\n",
    "\n",
    "## Step 11:  Test n = 7: 3(7) + 3 = 24, which is not a perfect cube.\n",
    "Testing n = 7, we get 3(7) + 3 = 24.\n",
    "\n",
    "## Step 12:  Test n = 8: 3(8) + 3 = 27, which is a perfect cube (3^3).\n",
    "Testing n = 8, we get 3(8) + 3 = 27, which is a perfect cube.\n",
    "\n",
    "## Step 13:  Conclusion.\n",
    "The smallest positive perfect cube that can be expressed as the sum of three consecutive integers is 27.\n",
    "\n",
    "The final answer is: $\\boxed{27}$\n",
    "\n",
    "\n",
    "Beam 3: last_score = 0.9922, completed = True, steps = 14\n",
    "Full response: ## Step 1:  Understand what a perfect cube is.\n",
    "A perfect cube is the cube of an integer, such as 1, 8, 27, etc.\n",
    "\n",
    "## Step 2:  Determine what the sum of three consecutive integers looks like.\n",
    "The sum of three consecutive integers can be represented as n + (n + 1) + (n + 2), where n is the first integer.\n",
    "\n",
    "## Step 3:  Express the sum of three consecutive integers mathematically.\n",
    "The sum can be simplified to 3n + 3.\n",
    "\n",
    "## Step 4:  Consider the requirements for the sum to be a perfect cube.\n",
    "We need to find the smallest value of n for which 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 5:  Start testing values of n to see if 3n + 3 is a perfect cube.\n",
    "We can start testing with n = 1: 3(1) + 3 = 6, which is not a perfect cube.\n",
    "\n",
    "## Step 6:  Continue testing values of n.\n",
    "We can test subsequent values for n until we find a value for which 3n + 3 is a perfect cube.\n",
    "\n",
    "## Step 7:  Find the first few perfect cubes and see which one can be written as 3n + 3.\n",
    "We can list a few perfect cubes to see if any can be expressed in the form 3n + 3.\n",
    "\n",
    "## Step 8:  Start testing with n = 4: 3(4) + 3 = 15, which is not a perfect cube.\n",
    "Testing n = 4, we get 3(4) + 3 = 15.\n",
    "\n",
    "## Step 9:  Continue testing with n = 5: 3(5) + 3 = 18, which is not a perfect cube.\n",
    "Testing n = 5, we get 3(5) + 3 = 18.\n",
    "\n",
    "## Step 10:  Keep testing with n = 6: 3(6) + 3 = 21, which is not a perfect cube.\n",
    "Testing n = 6, we get 3(6) + 3 = 21.\n",
    "\n",
    "## Step 11:  Test n = 7: 3(7) + 3 = 24, which is not a perfect cube.\n",
    "Testing n = 7, we get 3(7) + 3 = 24.\n",
    "\n",
    "## Step 12:  Test n = 8: 3(8) + 3 = 27, which is a perfect cube (3^3).\n",
    "Testing n = 8, we get 3(8) + 3 = 27, which is indeed a perfect cube.\n",
    "\n",
    "## Step 13:  We found a perfect cube, so we can conclude.\n",
    "The smallest positive perfect cube that can be written as the sum of three consecutive integers is 27.\n",
    "\n",
    "The final answer is: $\\boxed{27}$\n",
    "\n",
    "\n",
    "------------------------\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Conclusion\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In this lesson, we've explored several test-time compute algorithms that enhance LLM reasoning capabilities without changing the model weights:\n",
    "\n",
    "1. **Tree of Thoughts (ToT)**: Extends CoT reasoning by exploring multiple reasoning paths in a tree structure, allowing for systematic exploration with deliberate search algorithms like BFS or DFS.\n",
    "\n",
    "2. **Best-of-N Sampling**: A simple but effective approach that generates multiple independent reasoning paths and selects the best one based on evaluation by Outcome Reward Models or rule-based heuristics.\n",
    "\n",
    "3. **Weighted Best-of-N**: An enhancement to Best-of-N that considers both answer quality and frequency, providing more robust results for verifiable problems.\n",
    "\n",
    "4. **Beam Search with Process Reward Models**: A sophisticated approach that combines beam search with continuous feedback on intermediate reasoning steps, enabling more fine-grained control over the reasoning process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Key Takeaways**\n",
    "\n",
    "- Test-time compute offers a flexible and resource-efficient way to enhance LLM reasoning (compared with pre-training)\n",
    "- These approaches represent different tradeoffs between:\n",
    "  - Computational complexity\n",
    "  - Implementation difficulty\n",
    "  - Performance improvement\n",
    "  - Adaptability to different problem types\n",
    "\n",
    "- The effectiveness of these methods often depends on:\n",
    "  - The quality of evaluation models (ORM/PRM)\n",
    "  - Problem complexity and domain\n",
    "  - Computational resources available at inference time"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
